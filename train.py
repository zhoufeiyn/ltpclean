# 0920 update: try to overfit level1-1 in one directory
# æ›´æ–°: æ·»åŠ äº†æƒé‡ä¿å­˜å’Œç»§ç»­è®­ç»ƒåŠŸèƒ½

"""
ä½¿ç”¨è¯´æ˜:
1. æ­£å¸¸è®­ç»ƒ: åœ¨ config/configTrain.py ä¸­è®¾ç½® resume_training = False
   - ä¼šåŠ è½½é¢„è®­ç»ƒæ¨¡å‹ (cfg.model_path)

2. ç»§ç»­è®­ç»ƒ: åœ¨ config/configTrain.py ä¸­è®¾ç½®:
   - resume_training = True
   - resume_checkpoint_path = "ckpt/model_epoch100_20251018_19.pth"  # æŒ‡å®šè¦åŠ è½½çš„checkpointè·¯å¾„
   - ä¼šä¼˜å…ˆåŠ è½½ç»§ç»­è®­ç»ƒçš„checkpointï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ°é¢„è®­ç»ƒæ¨¡å‹

æƒé‡åŠ è½½ä¼˜å…ˆçº§:
1. ç»§ç»­è®­ç»ƒcheckpoint (åŒ…å«æ¨¡å‹æƒé‡+ä¼˜åŒ–å™¨çŠ¶æ€+è®­ç»ƒä¿¡æ¯)
2. é¢„è®­ç»ƒæ¨¡å‹ (ä»…åŒ…å«æ¨¡å‹æƒé‡)
3. éšæœºåˆå§‹åŒ–æ¨¡å‹

ä¿å­˜çš„æ¨¡å‹åŒ…å«:
- æ¨¡å‹æƒé‡ (network_state_dict)
- ä¼˜åŒ–å™¨çŠ¶æ€ (optimizer_state_dict)
- è®­ç»ƒä¿¡æ¯ (epochs, loss, model_nameç­‰)

è‡ªåŠ¨ä¿å­˜:
- å®šæœŸcheckpoint: model_epoch{epoch}_{timestamp}.pth
"""

from models.vae.sdvae import SDVAE
from algorithm import Algorithm
import torch
import config.configTrain as cfg
import matplotlib.pyplot as plt
import os
from datetime import datetime
from infer_test import model_test
import logging
# å¯¼å…¥æ•°æ®åŠ è½½æ¨¡å—
from dataloader.dataLoad import MarioDataset
from torch.utils.data import DataLoader

device: str = "cuda" if torch.cuda.is_available() else "cpu"


# -----------------------------
# Logging Setup
# -----------------------------
def setup_logging():
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    # åˆ›å»ºlogsç›®å½•
    log_dir = "logs"
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)

    # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H")
    log_filename = f"training_log_{timestamp}.log"
    log_path = os.path.join(log_dir, log_filename)

    # é…ç½®æ—¥å¿—æ ¼å¼
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_path, encoding='utf-8'),
            logging.StreamHandler()  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
        ]
    )

    logger = logging.getLogger(__name__)
    logger.info(f"init log: {log_path}")
    return logger, log_path


def save_loss_curve(loss_history, data_save_epoch, save_path="output"):
    """ä¿å­˜æŸå¤±æ›²çº¿å›¾"""
    if not os.path.exists(save_path):
        os.makedirs(save_path)

    # åˆ›å»ºæŸå¤±æ›²çº¿å›¾
    plt.figure(figsize=(10, 6))
    x_epochs = [(i + 1) * data_save_epoch for i in range(len(loss_history))]
    plt.plot(x_epochs, loss_history, 'b-', linewidth=2, label='Training Loss')
    plt.title('Training Loss Curve', fontsize=16)
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss', fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.legend()

    # ä¿å­˜å›¾ç‰‡
    timestamp = datetime.now().strftime("%Y%m%d_%H")
    loss_curve_path = os.path.join(save_path, f"loss_curve_{timestamp}.png")
    plt.savefig(loss_curve_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"ğŸ“ˆ Loss curve saved to: {loss_curve_path}")
    return loss_curve_path


# -----------------------------
# Model Saving Function
# -----------------------------
def save_model(model, optimizer, epochs, final_loss, path=cfg.ckpt_path):
    """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹åˆ°ckptç›®å½•"""

    if not os.path.exists(path):
        os.makedirs(path)

    # ç”Ÿæˆæ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³å’Œepochä¿¡æ¯ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H")
    model_filename = f"model_epoch{epochs}_{timestamp}.pth"
    model_path = os.path.join(path, model_filename)

    # å‡†å¤‡ä¿å­˜çš„æ•°æ®
    save_data = {
        'network_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'epochs': epochs,
        'loss': final_loss,
        'model_name': cfg.model_name,
        'batch_size': cfg.batch_size,
        'num_frames': cfg.num_frames,
        'timestamp': timestamp,
    }

    # ä¿å­˜æ¨¡å‹
    try:
        torch.save(save_data, model_path)
        print(f"âœ… save model to {model_path}")

    except Exception as e:
        print(f"âŒ save model failed: {e}")


def load_model(model, optimizer, checkpoint_path, device_obj):
    """åŠ è½½æ¨¡å‹æƒé‡å’Œä¼˜åŒ–å™¨çŠ¶æ€"""
    if not os.path.exists(checkpoint_path):
        print(f"âš ï¸ Checkpoint not found: {checkpoint_path}")
        return 0, float('inf')

    try:
        print(f"ğŸ“¥ Loading checkpoint: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=device_obj, weights_only=False)

        # åŠ è½½æ¨¡å‹æƒé‡
        model.load_state_dict(checkpoint['network_state_dict'], strict=False)
        print("âœ… Model weights loaded successfully!")

        # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
        if 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            print("âœ… Optimizer state loaded successfully!")
        else:
            print("âš ï¸ No optimizer state found in checkpoint")

        # è·å–è®­ç»ƒä¿¡æ¯
        start_epoch = checkpoint.get('epochs', 0)
        best_loss = checkpoint.get('loss', float('inf'))

        print(f"ğŸ“Š Loaded checkpoint info:")
        print(f"   - Epoch: {start_epoch}")
        print(f"   - Loss: {best_loss:.6f}")
        print(f"   - Model: {checkpoint.get('model_name', 'Unknown')}")

        return start_epoch, best_loss

    except Exception as e:
        print(f"âŒ Failed to load checkpoint: {e}")
        return 0, float('inf')


def vae_encode(batch_data_images, vae_model, device, scale_factor=0.1355):
    """vae encode the images"""
    # å°†å›¾åƒç¼–ç åˆ°æ½œåœ¨ç©ºé—´: [batch_size, num_frames, 3, 128, 128] -> [batch_size, num_frames, 4, 32, 32]
    with torch.no_grad():
        batch_size_videos, num_frames, channels, h, w = batch_data_images.shape
        # é‡å¡‘ä¸º [batch_size * num_frames, 3, 128, 128] è¿›è¡Œæ‰¹é‡ç¼–ç 
        images_flat = batch_data_images.reshape(-1, channels, h, w).to(device)

        # VAEç¼–ç 
        if vae_model is not None:
            latent_dist = vae_model.encode(images_flat)  # [batch_size * num_frames, 3, 128, 128]
            latent_images = latent_dist.sample()  # é‡‡æ ·æ½œåœ¨è¡¨ç¤º [batch_size * num_frames, 4, 32, 32]

            latent_images = latent_images * scale_factor
            # print(f"   Using scale factor: {Config.scale_factor}")

            # é‡å¡‘å› [batch_size, num_frames, 4, 32, 32]
            latent_images = latent_images.reshape(batch_size_videos, num_frames, 4, 32,
                                                  32)  # [batch_size, num_frames, 4, 32, 32]
        else:
            print("âš ï¸ Cannot find VAE model, use original image")
            # å¦‚æœæ²¡æœ‰VAEï¼Œç›´æ¥ä½¿ç”¨åŸå§‹å›¾åƒï¼Œä½†éœ€è¦è°ƒæ•´å½¢çŠ¶
            latent_images = images_flat.reshape(batch_size_videos, num_frames, channels, h, w)
            print(f"   Using original image shape: {latent_images.shape}")

        # æ›´æ–°batch_data[0]ä¸ºç¼–ç åçš„æ½œåœ¨è¡¨ç¤ºï¼Œä¿æŒåœ¨GPUä¸Š
        return latent_images


def train():
    # åˆå§‹åŒ–æ—¥å¿—è®°å½•
    logger, log_path = setup_logging()

    device_obj = torch.device(device)
    # ä½¿ç”¨å¤šè¿›ç¨‹æ•°æ®åŠ è½½ä¼˜åŒ–
    dataset = MarioDataset(cfg)
    dataloder = DataLoader(dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)

    # æ‰“å°æ•°æ®é›†ä¿¡æ¯ï¼ˆåŒ…æ‹¬è·³å¸§æ•ˆæœï¼‰
    logger.info(f"ğŸ“Š Dataset loaded: {len(dataset)} samples")
    logger.info(f"ğŸ“Š Frame sampling threshold (train_sample): {cfg.train_sample}")

    # video sequence parameters
    num_frames = cfg.num_frames
    model_name = cfg.model_name

    loss_log_iter = cfg.loss_log_iter
    # gif_save_iter = cfg.gif_save_iter
    gif_save_epoch = cfg.gif_save_epoch
    checkpoint_save_epoch = cfg.checkpoint_save_epoch

    # ä½¿ç”¨Algorithmç±»åŠ è½½å®Œæ•´çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆåŒ…å«VAEå’ŒDiffusionï¼‰
    model = Algorithm(model_name, device_obj)
    model = model.to(device_obj)
    # diffusion_model = model.df_model

    opt = model.df_model.configure_optimizers_gpt()

    # åˆå§‹åŒ–è®­ç»ƒçŠ¶æ€
    start_epoch = 0
    best_loss = float('inf')

    # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­è®­ç»ƒ - ä¼˜å…ˆåŠ è½½ç»§ç»­è®­ç»ƒçš„checkpoint
    if cfg.resume_training and cfg.resume_checkpoint_path:
        print(f"ğŸ”„ Resuming training from checkpoint: {cfg.resume_checkpoint_path}")
        start_epoch, best_loss = load_model(model, opt, cfg.resume_checkpoint_path, device_obj)
        if start_epoch > 0:
            print(f"âœ… Resuming training from epoch {start_epoch}")
        else:
            print("âš ï¸ Failed to load resume checkpoint, falling back to pretrained model")
            # å¦‚æœç»§ç»­è®­ç»ƒåŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°é¢„è®­ç»ƒæ¨¡å‹
            checkpoint_path = os.path.join(cfg.ckpt_path, cfg.model_path)
            if os.path.exists(checkpoint_path):
                print(f"ğŸ“¥ Loading diffusion forcing pretrained checkpoint: {checkpoint_path}")
                state_dict = torch.load(checkpoint_path, map_location=device_obj, weights_only=False)
                model.load_state_dict(state_dict['network_state_dict'], strict=False)
                print("âœ… diffusion forcing Pretrained checkpoint loaded successfully!")
            else:
                print(
                    f"âš ï¸ diffusion forcing pretrained checkpoint not found: {checkpoint_path}, using random initialized model")
    else:
        # æ²¡æœ‰è®¾ç½®ç»§ç»­è®­ç»ƒï¼ŒåŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        checkpoint_path = os.path.join(cfg.ckpt_path, cfg.model_path)
        if os.path.exists(checkpoint_path):
            print(f"ğŸ“¥ Loading diffusion forcing pretrained checkpoint: {checkpoint_path}")
            state_dict = torch.load(checkpoint_path, map_location=device_obj, weights_only=False)
            model.load_state_dict(state_dict['network_state_dict'], strict=False)
            print("âœ… diffusion forcing pretrained checkpoint loaded successfully!")
        else:
            print(
                f"âš ï¸ diffusion forcing pretrained checkpoint not found: {checkpoint_path}, using random initialized model")
        print("ğŸ†• Starting fresh training")

    # è·å–VAEå’ŒDiffusionæ¨¡å‹
    vae = SDVAE().to(device_obj)

    # åŠ è½½æ‚¨è‡ªå·±è®­ç»ƒçš„VAEæƒé‡
    custom_vae_path = cfg.vae_model
    if custom_vae_path and os.path.exists(custom_vae_path):
        print(f"ğŸ“¥ load your own vae ckpt: {custom_vae_path}")
        custom_state_dict = torch.load(custom_vae_path, map_location=device_obj)
        vae.load_state_dict(custom_state_dict['network_state_dict'], strict=False)
        print("âœ… your vae ckpt loaded successfullyï¼")
    else:
        print("â„¹ï¸ use default pre-trained vae ckpt")

    if vae is not None:
        vae.eval()
        for param in vae.parameters():  # freeze VAE parameters
            param.requires_grad = False
        print("âœ… VAE already loadedï¼ŒVAE parameters has been frozen")
    else:
        print("âš ï¸ Cannot find VAE model")
    epochs, batch_size = cfg.epochs, cfg.batch_size
    gradient_accumulation_steps = cfg.gradient_accumulation_steps

    print("---1. start training----")
    print("---2. load dataset---")
    total_video_sequences = len(dataset)  # dataset å·²ç»è¿”å›æœ‰æ•ˆåºåˆ—æ•°é‡
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®
    if total_video_sequences < 1:
        print(f"âŒ dataset not enough: no valid video sequences")
        return

    num_batches = (total_video_sequences + batch_size - 1) // batch_size
    print(f"ğŸ“Š Dataset info:")
    print(f"   - valid video sequences: {total_video_sequences}")
    print(f"   - each video has {num_frames} frames")
    print(f"   - batch size: {batch_size}")
    print(f"   - batches per epoch: {num_batches}")
    print(
        f"   - gradient accumulation steps: {gradient_accumulation_steps} (effective batch size: {batch_size * gradient_accumulation_steps})")

    # åˆå§‹åŒ–æŸå¤±å†å²è®°å½•
    loss_history = []
    final_avg_loss = 0  # ç”¨äºä¿å­˜æœ€ç»ˆçš„avg_loss
    avg_loss = 0  # åˆå§‹åŒ–avg_lossï¼Œé¿å…UnboundLocalError

    for epoch in range(start_epoch, epochs):
        total_loss = 0
        batch_count = 0

        # æ¢¯åº¦ç´¯ç§¯ç›¸å…³å˜é‡
        accumulation_step = 0

        # ç¡®ä¿æ¯ä¸ªepochå¼€å§‹æ—¶optimizerçš„æ¢¯åº¦æ˜¯æ¸…é›¶çš„
        opt.zero_grad()

        for batch_data in dataloder:
            batch_images, batch_actions, batch_nonterminals = batch_data
            batch_data = [
                batch_images.to(device_obj),
                batch_actions.to(device_obj),
                batch_nonterminals.to(device_obj)
            ]
            batch_data[0] = vae_encode(batch_data[0], vae, device_obj)

            # å°†batché‡å¤2éï¼Œå¢åŠ æ•°æ®é‡
            # repeat(2, 1, 1, ...) è¡¨ç¤ºåœ¨ç¬¬ä¸€ä¸ªç»´åº¦ï¼ˆbatchç»´åº¦ï¼‰é‡å¤2æ¬¡ï¼Œå…¶ä»–ç»´åº¦ä¸å˜
            batch_data[0] = batch_data[0].repeat(14, 1, 1, 1, 1)  # images: [batch, frames, C, H, W]
            batch_data[1] = batch_data[1].repeat(14, 1, 1)  # actions: [batch, frames, 1]
            batch_data[2] = batch_data[2].repeat(14, 1)  # nonterminals: [batch, frames]

            try:
                out_dict = model.df_model.training_step(batch_data)
                loss = out_dict["loss"]  # ç”¨lossè¿˜æ˜¯original_loss??

                # å°†lossé™¤ä»¥ç´¯ç§¯æ­¥æ•°ï¼Œä»¥ä¾¿æ¢¯åº¦ç´¯ç§¯åç­‰ä»·äºæ›´å¤§çš„batch size
                loss = loss / gradient_accumulation_steps

                # åå‘ä¼ æ’­ï¼ˆç´¯ç§¯æ¢¯åº¦ï¼‰
                # PyTorchçš„backward()ä¼šå°†è®¡ç®—å‡ºçš„æ¢¯åº¦åŠ åˆ°å‚æ•°çš„.gradå±æ€§ä¸Šï¼ˆç´¯åŠ è€Œéæ›¿æ¢ï¼‰
                # è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæ¢¯åº¦ä¼šè‡ªåŠ¨ç´¯ç§¯çš„åŸå› 
                loss.backward()

                accumulation_step += 1

                # å½“ç´¯ç§¯æ­¥æ•°è¾¾åˆ°è®¾å®šå€¼æ—¶ï¼Œæ‰§è¡Œä¼˜åŒ–å™¨æ›´æ–°
                if accumulation_step % gradient_accumulation_steps == 0:
                    opt.step()  # æ‰§è¡Œå‚æ•°æ›´æ–°
                    opt.zero_grad()  # æ¸…é›¶æ¢¯åº¦ï¼Œå‡†å¤‡ä¸‹ä¸€è½®ç´¯ç§¯

                total_loss += loss.item() * gradient_accumulation_steps  # ä¿®æ­£ç´¯ç§¯æŸå¤±å€¼
                batch_count += 1


            except Exception as e:
                print(f"   âŒ error in training step: {e}")
                print(f"   batch_data shapes:")
                print(f"     images: {batch_data[0].shape}")
                print(f"     actions: {batch_data[1].shape}")
                print(f"     nonterminals: {batch_data[2].shape}")
                raise e

            # æŸ¥çœ‹batché‡Œçš„losså’Œ gif
            if batch_count % loss_log_iter == 0:
                batch_loss = loss.item() * gradient_accumulation_steps  # ä¿®æ­£æ˜¾ç¤ºæŸå¤±å€¼
                loss_message = f"Epoch {epoch + 1}/{epochs}, in batch: {batch_count},  Loss: {batch_loss:.6f}"
                logger.info(loss_message)

        # epochç»“æŸæ—¶ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å‰©ä½™çš„æœªæ›´æ–°çš„æ¢¯åº¦
        if accumulation_step % gradient_accumulation_steps != 0:
            logger.info(
                f"Epoch {epoch + 1} ended with {accumulation_step % gradient_accumulation_steps} accumulated gradients, applying remaining update...")
            opt.step()
            opt.zero_grad()

        # # 5ä¸ªepoch
        if batch_count > 0 and (epoch + 1) % 5 == 0:
            # if batch_count > 0:
            avg_loss = total_loss / batch_count
            # scheduler.step(avg_loss)
            final_avg_loss = avg_loss  # æ›´æ–°æœ€ç»ˆçš„avg_loss
            # æ¯ 1 ä¸ªepochæ‰“å°ä¸€æ¬¡æŸå¤±å¹¶è®°å½•åˆ°å†å²
            loss_history.append(avg_loss)  # åªè®°å½•æ‰“å°çš„æŸå¤±å€¼
            loss_message = f"Epoch {epoch + 1}/{epochs}, Average Loss: {avg_loss:.6f}"
            logger.info(loss_message)
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™ä¿å­˜æœ€ä½³æ¨¡å‹
            is_best = avg_loss < best_loss
            if is_best:
                # ç«‹å³æ›´æ–°æœ€ä½³æŸå¤±
                improvement = (best_loss - avg_loss) / best_loss if best_loss != float('inf') else 1.0
                best_loss = avg_loss
                best_message = f"This is the new best loss(improvement: {improvement:.2%})"
                logger.info(best_message)

        # æ¯gif_save_epochä¸ªepoch runä¸€æ¬¡test,ä¿å­˜ gif
        if (epoch + 1) % gif_save_epoch == 0:
            # ç¡®ä¿outputç›®å½•å­˜åœ¨
            # model_test(cfg.test_img_path1, cfg.actions1, model, vae, device_obj, cfg.sample_step,
            #            f'{cfg.test_img_path1[-9:-4]}_epoch{epoch + 1}_r', epoch=epoch + 1, output_dir=cfg.out_dir)
            model_test(cfg.test_img_path2, cfg.actions2, model, vae, device_obj, cfg.sample_step,
                       f'{cfg.test_img_path2[-9:-4]}_epoch{epoch + 1}_rj', epoch=epoch + 1, output_dir=cfg.out_dir)

            ## large data
            # model_test(cfg.test_img_path1, cfg.actions1, model, vae, device_obj, cfg.sample_step,
            #            f'{cfg.test_img_path1[-9:-4]}_epoch{epoch + 1}_r', epoch=epoch + 1, output_dir=cfg.out_dir)
            # model_test(cfg.test_img_path1, cfg.actions2, model, vae, device_obj, cfg.sample_step,
            #            f'{cfg.test_img_path1[-9:-4]}_epoch{epoch + 1}_rj', epoch=epoch + 1, output_dir=cfg.out_dir)
            # model_test(cfg.test_img_path2, cfg.actions1, model, vae, device_obj, cfg.sample_step,
            #            f'{cfg.test_img_path2[-9:-4]}_epoch{epoch + 1}_r', epoch=epoch + 1, output_dir=cfg.out_dir)
            # model_test(cfg.test_img_path2, cfg.actions2, model, vae, device_obj, cfg.sample_step,
            #            f'{cfg.test_img_path2[-9:-4]}_epoch{epoch + 1}_rj', epoch=epoch + 1, output_dir=cfg.out_dir)
            # model_test(cfg.test_img_path3, cfg.actions1, model, vae, device_obj, cfg.sample_step,
            #            f'{cfg.test_img_path3[-9:-4]}_epoch{epoch + 1}_r', epoch=epoch + 1, output_dir=cfg.out_dir)
            # model_test(cfg.test_img_path3, cfg.actions2, model, vae, device_obj, cfg.sample_step,
            #            f'{cfg.test_img_path3[-9:-4]}_epoch{epoch + 1}_rj', epoch=epoch + 1, output_dir=cfg.out_dir)
            # model_test(cfg.test_img_path4, cfg.actions1, model, vae, device_obj, cfg.sample_step,
            #            f'{cfg.test_img_path4[-9:-4]}_epoch{epoch + 1}_r', epoch=epoch + 1, output_dir=cfg.out_dir)
            # model_test(cfg.test_img_path4, cfg.actions2, model, vae, device_obj, cfg.sample_step,
            #            f'{cfg.test_img_path4[-9:-4]}_epoch{epoch + 1}_rj', epoch=epoch + 1, output_dir=cfg.out_dir)

        # æ¯checkpoint_save_epochä¸ªepochä¿å­˜ä¸€æ¬¡checkpoint
        if (epoch + 1) % checkpoint_save_epoch == 0:
            current_loss = avg_loss if batch_count > 0 else 0
            save_model(model, opt, epoch + 1, current_loss, path=cfg.ckpt_path)
            checkpoint_message = f"ğŸ’¾ Checkpoint saved at epoch {epoch + 1}"
            logger.info(checkpoint_message)

    completion_message = "Training completed!"
    print(completion_message)
    logger.info(completion_message)

    # è®­ç»ƒå®Œæˆåä¿å­˜æœ€ç»ˆæ¨¡å‹
    if epochs >= 1 and final_avg_loss > 0:
        save_message = "ğŸ’¾ save final training model..."
        print(save_message)
        logger.info(save_message)

        save_model(model, opt, epochs, final_avg_loss, path=cfg.ckpt_path)

        # è®°å½•è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        stats_message = f"ğŸ“Š training statistics: total epochs: {epochs}, best loss: {best_loss:.6f}, final loss: {final_avg_loss:.6f}, total batches: {batch_count * epochs}"
        print(f"ğŸ“Š training statistics:")
        print(f"    total epochs: {epochs}")
        print(f"    best loss: {best_loss:.6f}")
        print(f"    final loss: {final_avg_loss:.6f}")
        print(f"    total batches: {batch_count * epochs}")
        logger.info(stats_message)

        # model_test(cfg.test_img_path1, cfg.actions1, model, vae, device_obj, cfg.sample_step,
        #            f'{cfg.test_img_path1[-9:-4]}_epoch{epoch + 1}_r', epoch='result', output_dir=cfg.out_dir)
        model_test(cfg.test_img_path2, cfg.actions2, model, vae, device_obj, cfg.sample_step,
                   f'{cfg.test_img_path2[-9:-4]}_epoch{epoch + 1}_rj', epoch='result', output_dir=cfg.out_dir)

        # # è®­ç»ƒå®Œæˆåè¿›è¡Œæµ‹è¯•
        # model_test(cfg.test_img_path1, cfg.actions1, model, vae, device_obj, cfg.sample_step,
        #            f'{cfg.test_img_path1[-9:-4]}_result_{epochs}_r', epoch='result', output_dir=cfg.out_dir)
        # model_test(cfg.test_img_path1, cfg.actions2, model, vae, device_obj, cfg.sample_step,
        #            f'{cfg.test_img_path1[-9:-4]}_result_{epochs}_rj', epoch='result', output_dir=cfg.out_dir)
        # model_test(cfg.test_img_path2, cfg.actions1, model, vae, device_obj, cfg.sample_step,
        #            f'{cfg.test_img_path2[-9:-4]}_result_{epochs}_r', epoch='result', output_dir=cfg.out_dir)
        # model_test(cfg.test_img_path2, cfg.actions2, model, vae, device_obj, cfg.sample_step,
        #            f'{cfg.test_img_path2[-9:-4]}_result_{epochs}_rj', epoch='result', output_dir=cfg.out_dir)
        # model_test(cfg.test_img_path3, cfg.actions1, model, vae, device_obj, cfg.sample_step,
        #            f'{cfg.test_img_path3[-9:-4]}_epoch{epoch + 1}_r', epoch='result', output_dir=cfg.out_dir)
        # model_test(cfg.test_img_path3, cfg.actions2, model, vae, device_obj, cfg.sample_step,
        #            f'{cfg.test_img_path3[-9:-4]}_epoch{epoch + 1}_rj', epoch='result', output_dir=cfg.out_dir)
        # model_test(cfg.test_img_path4, cfg.actions1, model, vae, device_obj, cfg.sample_step,
        #            f'{cfg.test_img_path4[-9:-4]}_epoch{epoch + 1}_r', epoch=epoch + 1, output_dir=cfg.out_dir)
        # model_test(cfg.test_img_path4, cfg.actions2, model, vae, device_obj, cfg.sample_step,
        #            f'{cfg.test_img_path4[-9:-4]}_epoch{epoch + 1}_rj', epoch=epoch + 1, output_dir=cfg.out_dir)

    # ä¿å­˜æœ€ç»ˆæŸå¤±æ›²çº¿åˆ°outputç›®å½•
    if len(loss_history) > 0:
        final_loss_curve_path = save_loss_curve(loss_history, 1, save_path="output")
        logger.info(f"Final loss curve saved to: {final_loss_curve_path}")

    # è®°å½•æ—¥å¿—æ–‡ä»¶è·¯å¾„
    final_log_message = f"log path: {log_path}"
    print(final_log_message)
    logger.info(final_log_message)


if __name__ == "__main__":
    train()