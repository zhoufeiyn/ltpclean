# 0920 update: try to overfit level1-1 in one directory

from typing import Optional
import re
from models.vae.sdxlvae import SDXLVAE
from algorithm import Algorithm
import torch

from torch.utils.data import Dataset
from torchvision import transforms
import config.configTrain as cfg

import matplotlib.pyplot as plt
from PIL import Image

import os
from datetime import datetime
from infer_test import model_test
import logging



device: str = "cuda" if torch.cuda.is_available() else "cpu"

# -----------------------------
# Logging Setup
# -----------------------------
def setup_logging():
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    # åˆ›å»ºlogsç›®å½•
    log_dir = "logs"
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    
    # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H")
    log_filename = f"training_log_{timestamp}.log"
    log_path = os.path.join(log_dir, log_filename)
    
    # é…ç½®æ—¥å¿—æ ¼å¼
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_path, encoding='utf-8'),
            logging.StreamHandler()  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"init log: {log_path}")
    return logger, log_path

def save_loss_curve(loss_history, data_save_epoch, save_path="output"):
    """ä¿å­˜æŸå¤±æ›²çº¿å›¾"""
    if not os.path.exists(save_path):
        os.makedirs(save_path)
    
    # åˆ›å»ºæŸå¤±æ›²çº¿å›¾
    plt.figure(figsize=(10, 6))
    x_epochs = [(i + 1) * data_save_epoch for i in range(len(loss_history))]
    plt.plot(x_epochs, loss_history, 'b-', linewidth=2, label='Training Loss')
    plt.title('Training Loss Curve', fontsize=16)
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss', fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.legend()
    
    # ä¿å­˜å›¾ç‰‡
    timestamp = datetime.now().strftime("%Y%m%d_%H")
    loss_curve_path = os.path.join(save_path, f"loss_curve_{timestamp}.png")
    plt.savefig(loss_curve_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“ˆ Loss curve saved to: {loss_curve_path}")
    return loss_curve_path

# -----------------------------
# Model Saving Function
# -----------------------------
def save_model(model, epochs, final_loss,path=cfg.ckpt_path):
    """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹åˆ°ckptç›®å½•"""    

    if not os.path.exists(path):
        os.makedirs(path)

    
    # ç”Ÿæˆæ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³å’Œepochä¿¡æ¯ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H")
    model_filename = f"model_epoch{epochs}_{timestamp}.pth"
    model_path = os.path.join(path, model_filename)
    
    # å‡†å¤‡ä¿å­˜çš„æ•°æ®
    save_data = {
        'network_state_dict': model.state_dict(),
        'epochs': epochs,
        'loss': final_loss,
        'model_name': cfg.model_name,
        'batch_size': cfg.batch_size,
        'num_frames': cfg.num_frames,
    }
    
    # ä¿å­˜æ¨¡å‹
    try:
        torch.save(save_data, model_path)
        print(f"âœ… save model to {model_path}")

    except Exception as e:
        print(f"âŒ save model failed: {e}")

def save_best_checkpoint(model, epoch,  best_loss, is_best=False,path=cfg.ckpt_path):
    """ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆå®šæœŸä¿å­˜å’Œæœ€ä½³æ¨¡å‹ä¿å­˜ï¼‰"""


    if not os.path.exists(path):
        os.makedirs(path)
    
    # ä¿å­˜æœ€æ–°æ¨¡å‹
    if is_best:
        latest_path = os.path.join(path, "best_model.pth")
        checkpoint_data = {
            'network_state_dict': model.state_dict(),
            'epoch': epoch,
            'loss': best_loss,
            'model_name': cfg.model_name,
            'batch_size': cfg.batch_size,
            'num_frames': cfg.num_frames
        }
    
        torch.save(checkpoint_data, latest_path)
        print(f"ğŸ† save best model: epoch: {epoch},best loss = {best_loss:.6f}")
    

# -----------------------------
# Custom Dataset for Mario Data
# -----------------------------

class MarioDataset(Dataset):
    """load mario dataset __init__ action and img paths,
     __getitem__  will return image and corresponding action"""
    """up to date: 2025-09-20 only load all frames in one directory,
     return array ofimages and actions"""
    def __init__(self, data_path: str, image_size):
        self.data_path = data_path
        self.image_size = image_size
        self.image_files = [] # image files path (xxx.png)
        self.actions = [] # action (0-255)
        self._load_data()
        self.transform = transforms.Compose([
            transforms.Resize((image_size, image_size)),
            transforms.ToTensor(), # [0, 1]
            transforms.Normalize(0.5, 0.5),  # [-1, 1]
        ])
    def _load_data(self):
        """load all png files and corresponding actions"""
        print(f" data path is scanning: {self.data_path}")
        if not os.path.exists(self.data_path): 
            print(f"âŒ data path not found: {self.data_path}")
            return
        
        total_files = 0
        valid_files = 0

        for root, dirs, files in os.walk(self.data_path):
            if root == self.data_path:
                continue
            for file in files:
                if file.lower().endswith('.png'):
                    total_files += 1
                    file_path = os.path.join(root, file)
                    
                    # å°è¯•ä»æ–‡ä»¶åæå–åŠ¨ä½œ
                    action = self._extract_action_from_filename(file)
                    if action is not None:
                        self.image_files.append(file_path)
                        self.actions.append(action)
                        valid_files += 1
                    else:
                        print(f"âš ï¸ can't extract action from filename: {file}")
    
    def _map_action_to_playgenaction(self, action: int) -> int:
        """map action to playgenaction"""
        # 255 to 7
        if action == 20: # right + B = 4+16=running right
            return 1
        elif action == 148: # right + B + A = 4+16+128=running jump right
            return 2
        elif action == 48: # left + B = 32+16=running left
            return 3
        elif action == 176: # left + B + A = 32+16+128=running jump left
            return 4
        elif action == 128: # A = 128=jump
            return 5
        elif action == 16: # B = 16=fire or run
            return 6
        elif action == 0: # null
            return 0

            
        
    def _extract_action_from_filename(self, filename: str) -> Optional[int]:
        """extract action from filename"""
        # æ–‡ä»¶åæ ¼å¼: Rafael_dp2a9j4i_e6_1-1_f1000_a20_2019-04-13_20-13-16.win.png
        pattern = r'_a(\d+)_'
        match = re.search(pattern, filename)
        if match:
            action = int(match.group(1))
            action_mapped = self._map_action_to_playgenaction(action)
            return action_mapped
        return None
        
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        """get the data sample of the specified index"""
        if idx >= len(self.image_files):
            raise IndexError(f"Index {idx} out of range for dataset of size {len(self.image_files)}")
        
        # åŠ è½½å›¾åƒ
        image_path = self.image_files[idx]
        image = Image.open(image_path).convert('RGB')
        image = self.transform(image)
        
        # è·å–åŠ¨ä½œï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        action = self.actions[idx] if idx < len(self.actions) else 0
        
        return image, action


def build_video_sequence(dataset, start_idx, end_idx):
    """build video sequence in one batch from dataset, return [1, num_frames, ch, h, w]"""

    # å­˜å‚¨ä¸€ä¸ªè§†é¢‘åºåˆ—çš„æ•°æ®
    video_images = []  # å­˜å‚¨å½“å‰è§†é¢‘çš„num_frameså¸§å›¾åƒ
    video_actions = []  # å­˜å‚¨å½“å‰è§†é¢‘çš„num_framesä¸ªåŠ¨ä½œ
    video_nonterminals = []  # å­˜å‚¨å½“å‰è§†é¢‘çš„num_framesä¸ªnonterminals

    # æ„å»ºå½“å‰è§†é¢‘åºåˆ—
    for frame_idx in range(start_idx, end_idx):
        image, action = dataset[frame_idx]
        video_images.append(image)  # image shape: [3, 128, 128]
        video_actions.append(action)  # actionæ˜¯æ•´æ•°ï¼Œç›´æ¥ä½¿ç”¨
        video_nonterminals.append(True)  # å…ˆéƒ½é»˜è®¤True
    # è½¬æ¢ä¸ºtensorå¹¶ç»„ç»‡æˆç›®æ ‡æ ¼å¼
    # [num_frames, channels, h, w] = [num_frames, 3, 128, 128]
    images_tensor = torch.stack(video_images, dim=0)  # [num_frames, 3, 128, 128]
    images_tensor = images_tensor.unsqueeze(0)  # [1, num_frames, 3, 128, 128]

    # [batch_size, num_frames, action_dim] = [1, num_frames, 1]
    actions_tensor = torch.tensor(video_actions, dtype=torch.long)  # [num_frames]
    actions_tensor = actions_tensor.unsqueeze(0).unsqueeze(-1)  # [1, num_frames, 1]

    # [batch_size, num_frames] = [1, num_frames]
    nonterminals_tensor = torch.tensor(video_nonterminals, dtype=torch.bool)  # [num_frames]
    nonterminals_tensor = nonterminals_tensor.unsqueeze(0)  # [1, num_frames]

    # è¿”å›tensorè€Œä¸æ˜¯åˆ—è¡¨
    return images_tensor, actions_tensor, nonterminals_tensor

def vae_encode(batch_data_images, vae_model, device, scale_factor=0.18215):
    """vae encode the images"""
    # å°†å›¾åƒç¼–ç åˆ°æ½œåœ¨ç©ºé—´: [batch_size, num_frames, 3, 128, 128] -> [batch_size, num_frames, 4, 32, 32]
    with torch.no_grad():
        batch_size_videos, num_frames, channels, h, w = batch_data_images.shape
        # é‡å¡‘ä¸º [batch_size * num_frames, 3, 128, 128] è¿›è¡Œæ‰¹é‡ç¼–ç 
        images_flat = batch_data_images.reshape(-1, channels, h, w).to(device)
        
        # VAEç¼–ç 
        if vae_model is not None:
            latent_dist = vae_model.encode(images_flat) # [batch_size * num_frames, 3, 128, 128]
            latent_images = latent_dist.sample()  # é‡‡æ ·æ½œåœ¨è¡¨ç¤º [batch_size * num_frames, 4, 32, 32]


            latent_images = latent_images * scale_factor
            # print(f"   Using scale factor: {Config.scale_factor}")
            
            # é‡å¡‘å› [batch_size, num_frames, 4, 32, 32]
            latent_images = latent_images.reshape(batch_size_videos, num_frames, 4, 32, 32) #[batch_size, num_frames, 4, 32, 32]
        else:
            print("âš ï¸ Cannot find VAE model, use original image")
            # å¦‚æœæ²¡æœ‰VAEï¼Œç›´æ¥ä½¿ç”¨åŸå§‹å›¾åƒï¼Œä½†éœ€è¦è°ƒæ•´å½¢çŠ¶
            latent_images = images_flat.reshape(batch_size_videos, num_frames, channels, h, w)
            print(f"   Using original image shape: {latent_images.shape}")
        
        # æ›´æ–°batch_data[0]ä¸ºç¼–ç åçš„æ½œåœ¨è¡¨ç¤ºï¼Œä¿æŒåœ¨GPUä¸Š
        return latent_images

def train():
    # åˆå§‹åŒ–æ—¥å¿—è®°å½•
    logger, log_path = setup_logging()
    
    device_obj = torch.device(device)
    dataset = MarioDataset(cfg.data_path, cfg.img_size)

    # video sequence parameters
    num_frames = cfg.num_frames
    batch_size = cfg.batch_size


    model_name = cfg.model_name

    best_save_interval = cfg.best_save_interval
    data_save_epoch = cfg.data_save_epoch
    gif_save_epoch = cfg.gif_save_epoch
    
    # ä½¿ç”¨Algorithmç±»åŠ è½½å®Œæ•´çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆåŒ…å«VAEå’ŒDiffusionï¼‰
    model = Algorithm(model_name, device_obj)
    
    # åŠ è½½é¢„è®­ç»ƒcheckpoint
    checkpoint_path = os.path.join(cfg.ckpt_path, cfg.model_path)
    if os.path.exists(checkpoint_path):
        print(f"ğŸ“¥ load pretrained checkpoint: {checkpoint_path}")
        state_dict = torch.load(checkpoint_path, map_location=device_obj, weights_only=False)
        model.load_state_dict(state_dict['network_state_dict'], strict=False)
        print("âœ… Checkpoint loaded successfullyï¼")

    else:
        print(f"âš ï¸ Checkpoint not found: {checkpoint_path},use random initialized model")
    model = model.to(device_obj)
    # model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œä½†å…è®¸è®­ç»ƒ
    
    # è·å–VAEå’ŒDiffusionæ¨¡å‹
    vae = SDXLVAE().to(device_obj)
    model.vae = vae
    diffusion_model = model.df_model

    
    if vae is not None:
        vae.eval()
        for param in vae.parameters(): # freeze VAE parameters
            param.requires_grad = False
        print("âœ… VAE already loadedï¼ŒVAE parameters has been frozen")
    else:
        print("âš ï¸ Cannot find VAE model")
    epochs, batch_size = cfg.epochs, cfg.batch_size
    

    opt = diffusion_model.configure_optimizers_gpt()
    

    print("---1. start training----")
    print("---2. load dataset---")
    total_samples = len(dataset)
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®
    if total_samples < num_frames:
        print(f"âŒ dataset not enough: need at least {num_frames} samples, but only {total_samples} samples")
        return
    # è®¡ç®—å¯ä»¥åˆ›å»ºå¤šå°‘ä¸ªå®Œæ•´çš„è§†é¢‘åºåˆ—
    num_videos = total_samples // num_frames
    print(f"dataset loaded: {total_samples} samples, construct {num_videos} complete video sequences, each video has {num_frames} frames, construct {num_videos//batch_size} batches, each batch has {batch_size} videos")
    
    # åˆå§‹åŒ–æœ€ä½³æŸå¤±è·Ÿè¸ª
    best_loss = float('inf')
    min_improvement = cfg.min_improvement  # æœ€å°æ”¹å–„å¹…åº¦
    final_avg_loss = 0  # ç”¨äºä¿å­˜æœ€ç»ˆçš„avg_loss
    
    # åˆå§‹åŒ–æŸå¤±å†å²è®°å½•
    loss_history = []  
    
    for epoch in range(epochs):
        total_loss = 0
        batch_count = 0
        avg_loss = 0

        # éå†æ‰€æœ‰è§†é¢‘åºåˆ—
        for i in range(0, total_samples, batch_size*num_frames):
            
            batch_images = []
            batch_actions = []
            batch_nonterminals = []
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®æ„å»ºå®Œæ•´æ‰¹æ¬¡
            if i + batch_size*num_frames > total_samples:
                print(f"âš ï¸ jump to next batch: need {batch_size*num_frames} samples, but only {total_samples - i} samples left")
                break
            
            for batch_idx in range(batch_size):
                start_idx = i + batch_idx*num_frames
                end_idx = start_idx + num_frames
                
                # ç¡®ä¿ä¸è¶…å‡ºæ•°æ®é›†è¾¹ç•Œ
                if end_idx > total_samples:
                    print(f"âš ï¸ jump to next batch: start_idx={start_idx}, end_idx={end_idx}, total_samples={total_samples}")
                    break
                    
                video_images, video_actions, video_nonterminals = build_video_sequence(dataset, start_idx, end_idx)
                
                # æ·»åŠ åˆ°æ‰¹æ¬¡åˆ—è¡¨ä¸­
                batch_images.append(video_images)
                batch_actions.append(video_actions)
                batch_nonterminals.append(video_nonterminals)
            
            # æ‹¼æ¥æˆbatch_tensor: [batch_size, num_frames, c, h, w]
            batch_data = [
                torch.cat(batch_images, dim=0).to(device_obj),
                torch.cat(batch_actions, dim=0).to(device_obj),
                torch.cat(batch_nonterminals, dim=0).to(device_obj)
            ]

           
            batch_data[0] = vae_encode(batch_data[0], vae, device_obj)
  
            
            # æ‰©å±•batch_size: [1, num_frames, channels, h, w] -> [16, num_frames, channels, h, w]
            batch_data[0] = batch_data[0].repeat(32, 1, 1, 1, 1)

            
            # åŒæ­¥æ‰©å±•actionså’Œnonterminals
            batch_data[1] = batch_data[1].repeat(32, 1, 1)  # actions: [1, num_frames, 1] -> [16, num_frames, 1]
            batch_data[2] = batch_data[2].repeat(32, 1)     # nonterminals: [1, num_frames] -> [16, num_frames]

            # è®­ç»ƒæ­¥éª¤
            try:
                out_dict = diffusion_model.training_step(batch_data)
                loss = out_dict["loss"]  # ç”¨lossè¿˜æ˜¯original_loss??
                
                # åå‘ä¼ æ’­
                opt.zero_grad()
                loss.backward()
                opt.step()
                
                total_loss += loss.item()
                batch_count += 1
                
                # if batch_count % 1 == 0:
                #     print(f"   Batch {batch_count}, Loss: {loss.item():.6f}") # print loss in every 1 batch
                
            except Exception as e:
                print(f"   âŒ error in training step: {e}")
                print(f"   batch_data shapes:")
                print(f"     images: {batch_data[0].shape}")
                print(f"     actions: {batch_data[1].shape}")
                print(f"     nonterminals: {batch_data[2].shape}")
                raise e        
        
        # è®¡ç®—æ¯ä¸ªepochçš„å¹³å‡æŸå¤±å¹¶è®°å½•
        if batch_count > 0:
            avg_loss = total_loss / batch_count
            # scheduler.step(avg_loss)
            final_avg_loss = avg_loss  # æ›´æ–°æœ€ç»ˆçš„avg_loss
            
            # æ¯ cfg.data_save_epoch ä¸ªepochæ‰“å°ä¸€æ¬¡æŸå¤±å¹¶è®°å½•åˆ°å†å²
            if (epoch+1) % data_save_epoch == 0:
                loss_history.append(avg_loss)  # åªè®°å½•æ‰“å°çš„æŸå¤±å€¼
                loss_message = f"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.6f}"
                
                logger.info(loss_message)

            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹ï¼Œå¦‚æœæ˜¯ï¼Œä¸”epoch> best_save_intervalï¼Œåˆ™ä¿å­˜æœ€ä½³æ¨¡å‹
                is_best = avg_loss < best_loss
                
                if is_best:
                    # ç«‹å³æ›´æ–°æœ€ä½³æŸå¤±
                    improvement = (best_loss - avg_loss) / best_loss if best_loss != float('inf') else 1.0
                    best_loss = avg_loss
                    best_message = f"This is the new best loss(improvement: {improvement:.2%})"
                    
                    logger.info(best_message)
                    
                    # æ£€æŸ¥æ˜¯å¦åœ¨ä¿å­˜é—´éš”å†…ä¸”æœ‰æ˜¾è‘—æ”¹å–„
                    if (epoch + 1) >= best_save_interval and improvement >= min_improvement:
                        save_best_checkpoint(model, epoch + 1, best_loss, is_best=True, path=cfg.ckpt_path)
                        save_best_message = f"save best model in {cfg.ckpt_path}(improvement: {improvement:.2%})"
                        
                        logger.info(save_best_message)
        
        # æ¯gif_save_epochä¸ªepoch runä¸€æ¬¡test,ä¿å­˜ gif
        if (epoch+1) % gif_save_epoch == 0:
            model_test(cfg.test_img_path, cfg.actions, model, device_obj, cfg.sample_step,epoch+1)

    
    completion_message = "Training completed!"
    print(completion_message)
    logger.info(completion_message)
    

    # è®­ç»ƒå®Œæˆåä¿å­˜æœ€ç»ˆæ¨¡å‹
    if epochs >= 1000 and final_avg_loss > 0:
        save_message = "ğŸ’¾ save final training model..."
        print(save_message)
        logger.info(save_message)
        
        save_model(model, epochs, final_avg_loss, path=cfg.ckpt_path)
        
        # è®°å½•è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        stats_message = f"ğŸ“Š training statistics: total epochs: {epochs}, best loss: {best_loss:.6f}, final loss: {final_avg_loss:.6f}, total batches: {batch_count * epochs}"
        print(f"ğŸ“Š training statistics:")
        print(f"    total epochs: {epochs}")
        print(f"    best loss: {best_loss:.6f}")
        print(f"    final loss: {final_avg_loss:.6f}")
        print(f"    total batches: {batch_count * epochs}")
        logger.info(stats_message)
        
        # è®­ç»ƒå®Œæˆåè¿›è¡Œæµ‹è¯•
        model_test(cfg.test_img_path, cfg.actions, model, device_obj, cfg.sample_step,epochs)
    
    # ä¿å­˜æœ€ç»ˆæŸå¤±æ›²çº¿åˆ°outputç›®å½•
    if len(loss_history) > 0:
        final_loss_curve_path = save_loss_curve(loss_history, data_save_epoch, save_path="output")
        logger.info(f"Final loss curve saved to: {final_loss_curve_path}")
    
    # è®°å½•æ—¥å¿—æ–‡ä»¶è·¯å¾„
    final_log_message = f"log path: {log_path}"
    print(final_log_message)
    logger.info(final_log_message)


    


if __name__ == "__main__":
    train()