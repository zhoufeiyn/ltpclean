# 0920 update: try to overfit level1-1 in one directory

from models.vae.sdxlvae import SDXLVAE
from algorithm import Algorithm
import torch
import config.configTrain as cfg
import matplotlib.pyplot as plt
import os
from datetime import datetime
from infer_test import model_test
import logging
# å¯¼å…¥æ•°æ®åŠ è½½æ¨¡å—
from dataLoad import MarioDataset, build_video_sequence_batch



device: str = "cuda" if torch.cuda.is_available() else "cpu"

# -----------------------------
# Logging Setup
# -----------------------------
def setup_logging():
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    # åˆ›å»ºlogsç›®å½•
    log_dir = "logs"
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    
    # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H")
    log_filename = f"training_log_{timestamp}.log"
    log_path = os.path.join(log_dir, log_filename)
    
    # é…ç½®æ—¥å¿—æ ¼å¼
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_path, encoding='utf-8'),
            logging.StreamHandler()  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"init log: {log_path}")
    return logger, log_path

def save_loss_curve(loss_history, data_save_epoch, save_path="output"):
    """ä¿å­˜æŸå¤±æ›²çº¿å›¾"""
    if not os.path.exists(save_path):
        os.makedirs(save_path)
    
    # åˆ›å»ºæŸå¤±æ›²çº¿å›¾
    plt.figure(figsize=(10, 6))
    x_epochs = [(i + 1) * data_save_epoch for i in range(len(loss_history))]
    plt.plot(x_epochs, loss_history, 'b-', linewidth=2, label='Training Loss')
    plt.title('Training Loss Curve', fontsize=16)
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss', fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.legend()
    
    # ä¿å­˜å›¾ç‰‡
    timestamp = datetime.now().strftime("%Y%m%d_%H")
    loss_curve_path = os.path.join(save_path, f"loss_curve_{timestamp}.png")
    plt.savefig(loss_curve_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“ˆ Loss curve saved to: {loss_curve_path}")
    return loss_curve_path

# -----------------------------
# Model Saving Function
# -----------------------------
def save_model(model, epochs, final_loss,path=cfg.ckpt_path):
    """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹åˆ°ckptç›®å½•"""    

    if not os.path.exists(path):
        os.makedirs(path)

    
    # ç”Ÿæˆæ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³å’Œepochä¿¡æ¯ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H")
    model_filename = f"model_epoch{epochs}_{timestamp}.pth"
    model_path = os.path.join(path, model_filename)
    
    # å‡†å¤‡ä¿å­˜çš„æ•°æ®
    save_data = {
        'network_state_dict': model.state_dict(),
        'epochs': epochs,
        'loss': final_loss,
        'model_name': cfg.model_name,
        'batch_size': cfg.batch_size,
        'num_frames': cfg.num_frames,
    }
    
    # ä¿å­˜æ¨¡å‹
    try:
        torch.save(save_data, model_path)
        print(f"âœ… save model to {model_path}")

    except Exception as e:
        print(f"âŒ save model failed: {e}")

def save_best_checkpoint(model, epoch,  best_loss, is_best=False,path=cfg.ckpt_path):
    """ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆå®šæœŸä¿å­˜å’Œæœ€ä½³æ¨¡å‹ä¿å­˜ï¼‰"""
    if not os.path.exists(path):
        os.makedirs(path)
    
    # ä¿å­˜æœ€æ–°æ¨¡å‹
    if is_best:
        latest_path = os.path.join(path, "best_model.pth")
        checkpoint_data = {
            'network_state_dict': model.state_dict(),
            'epoch': epoch,
            'loss': best_loss,
            'model_name': cfg.model_name,
            'batch_size': cfg.batch_size,
            'num_frames': cfg.num_frames
        }
    
        torch.save(checkpoint_data, latest_path)
        print(f"ğŸ† save best model: epoch: {epoch},best loss = {best_loss:.6f}")
    

def vae_encode(batch_data_images, vae_model, device, scale_factor=0.18215):
    """vae encode the images"""
    # å°†å›¾åƒç¼–ç åˆ°æ½œåœ¨ç©ºé—´: [batch_size, num_frames, 3, 128, 128] -> [batch_size, num_frames, 4, 32, 32]
    with torch.no_grad():
        batch_size_videos, num_frames, channels, h, w = batch_data_images.shape
        # é‡å¡‘ä¸º [batch_size * num_frames, 3, 128, 128] è¿›è¡Œæ‰¹é‡ç¼–ç 
        images_flat = batch_data_images.reshape(-1, channels, h, w).to(device)
        
        # VAEç¼–ç 
        if vae_model is not None:
            latent_dist = vae_model.encode(images_flat) # [batch_size * num_frames, 3, 128, 128]
            latent_images = latent_dist.sample()  # é‡‡æ ·æ½œåœ¨è¡¨ç¤º [batch_size * num_frames, 4, 32, 32]


            latent_images = latent_images * scale_factor
            # print(f"   Using scale factor: {Config.scale_factor}")
            
            # é‡å¡‘å› [batch_size, num_frames, 4, 32, 32]
            latent_images = latent_images.reshape(batch_size_videos, num_frames, 4, 32, 32) #[batch_size, num_frames, 4, 32, 32]
        else:
            print("âš ï¸ Cannot find VAE model, use original image")
            # å¦‚æœæ²¡æœ‰VAEï¼Œç›´æ¥ä½¿ç”¨åŸå§‹å›¾åƒï¼Œä½†éœ€è¦è°ƒæ•´å½¢çŠ¶
            latent_images = images_flat.reshape(batch_size_videos, num_frames, channels, h, w)
            print(f"   Using original image shape: {latent_images.shape}")
        
        # æ›´æ–°batch_data[0]ä¸ºç¼–ç åçš„æ½œåœ¨è¡¨ç¤ºï¼Œä¿æŒåœ¨GPUä¸Š
        return latent_images

def train():
    # åˆå§‹åŒ–æ—¥å¿—è®°å½•
    logger, log_path = setup_logging()
    
    device_obj = torch.device(device)
    # ä½¿ç”¨å¤šè¿›ç¨‹æ•°æ®åŠ è½½ä¼˜åŒ–
    dataset = MarioDataset(cfg.data_path, cfg.img_size, num_workers=8)

    # video sequence parameters
    num_frames = cfg.num_frames
    frame_interval = cfg.frame_interval
    model_name = cfg.model_name

    best_save_interval = cfg.best_save_interval
    data_save_epoch = cfg.data_save_epoch
    gif_save_epoch = cfg.gif_save_epoch
    
    # ä½¿ç”¨Algorithmç±»åŠ è½½å®Œæ•´çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆåŒ…å«VAEå’ŒDiffusionï¼‰
    model = Algorithm(model_name, device_obj)
    
    # åŠ è½½é¢„è®­ç»ƒcheckpoint
    checkpoint_path = os.path.join(cfg.ckpt_path, cfg.model_path)
    if os.path.exists(checkpoint_path):
        print(f"ğŸ“¥ load pretrained checkpoint: {checkpoint_path}")
        state_dict = torch.load(checkpoint_path, map_location=device_obj, weights_only=False)
        model.load_state_dict(state_dict['network_state_dict'], strict=False)
        print("âœ… Checkpoint loaded successfullyï¼")

    else:
        print(f"âš ï¸ Checkpoint not found: {checkpoint_path},use random initialized model")
    model = model.to(device_obj)

    # è·å–VAEå’ŒDiffusionæ¨¡å‹
    vae = SDXLVAE().to(device_obj)
    model.vae = vae
    diffusion_model = model.df_model

    if vae is not None:
        vae.eval()
        for param in vae.parameters(): # freeze VAE parameters
            param.requires_grad = False
        print("âœ… VAE already loadedï¼ŒVAE parameters has been frozen")
    else:
        print("âš ï¸ Cannot find VAE model")
    epochs, batch_size = cfg.epochs, cfg.batch_size
    

    opt = diffusion_model.configure_optimizers_gpt()
    

    print("---1. start training----")
    print("---2. load dataset---")
    total_samples = len(dataset)
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®
    if total_samples < num_frames:
        print(f"âŒ dataset not enough: need at least {num_frames} samples, but only {total_samples} samples")
        return
    # è®¡ç®—å¯ä»¥åˆ›å»ºå¤šå°‘ä¸ªå®Œæ•´çš„è§†é¢‘åºåˆ—
    num_videos = (total_samples-num_frames) // frame_interval + 1
    print(f"dataset loaded: {total_samples} samples, construct {num_videos} complete video sequences, "
        f"each video has {num_frames} frames, construct {(num_videos + batch_size - 1) // batch_size } batches, the batch size is {batch_size}")

    # åˆå§‹åŒ–æœ€ä½³æŸå¤±è·Ÿè¸ª
    best_loss = float('inf')
    min_improvement = cfg.min_improvement  # æœ€å°æ”¹å–„å¹…åº¦
    final_avg_loss = 0  # ç”¨äºä¿å­˜æœ€ç»ˆçš„avg_loss

    # åˆå§‹åŒ–æŸå¤±å†å²è®°å½•
    loss_history = []

    # é¢„è®¡ç®—æ‰€æœ‰æœ‰æ•ˆçš„è§†é¢‘åºåˆ—èµ·å§‹ä½ç½®
    valid_starts = []
    for start in range(0, total_samples - num_frames + 1, frame_interval):
        valid_starts.append(start)
    
    # æŒ‰batch_sizeåˆ†ç»„å¤„ç†
    num_valid_videos = len(valid_starts)
    
    for epoch in range(epochs):
        total_loss = 0
        batch_count = 0
        avg_loss = 0

        # æŒ‰batchå¤„ç† - ä¼˜åŒ–ç‰ˆæœ¬
        for batch_start in range(0, num_valid_videos, batch_size):
            batch_end = min(batch_start + batch_size, num_valid_videos)
            current_batch_size = batch_end - batch_start
            
            # è·å–å½“å‰batchçš„èµ·å§‹ç´¢å¼•
            current_start_indices = valid_starts[batch_start:batch_end]
            
            # æ‰¹é‡æ„å»ºè§†é¢‘åºåˆ—
            batch_images, batch_actions, batch_nonterminals = build_video_sequence_batch(
                dataset, current_start_indices, num_frames
            )
            
            # å¦‚æœbatchä¸æ»¡ï¼Œç”¨æœ€åä¸€ä¸ªè§†é¢‘å¤åˆ¶è¡¥é½
            if current_batch_size < batch_size:
                last_video_images = batch_images[-1]
                last_video_actions = batch_actions[-1]
                last_video_nonterminals = batch_nonterminals[-1]
                
                for _ in range(batch_size - current_batch_size):
                    batch_images.append(last_video_images)
                    batch_actions.append(last_video_actions)
                    batch_nonterminals.append(last_video_nonterminals)

            # æ‹¼æ¥æˆbatch_tensor
            batch_data = [
                torch.cat(batch_images, dim=0).to(device_obj),
                torch.cat(batch_actions, dim=0).to(device_obj),
                torch.cat(batch_nonterminals, dim=0).to(device_obj)
            ]

           
            batch_data[0] = vae_encode(batch_data[0], vae, device_obj)
            
            # æ‰©å±•batch_size: [1, num_frames, channels, h, w] -> [16, num_frames, channels, h, w]
            batch_data[0] = batch_data[0].repeat(16, 1, 1, 1, 1)
            
            
            # åŒæ­¥æ‰©å±•actionså’Œnonterminals
            batch_data[1] = batch_data[1].repeat(16, 1, 1)  # actions: [1, num_frames, 1] -> [16, num_frames, 1]
            batch_data[2] = batch_data[2].repeat(16, 1)     # nonterminals: [1, num_frames] -> [16, num_frames]

            # è®­ç»ƒæ­¥éª¤
            try:
                out_dict = diffusion_model.training_step(batch_data)
                loss = out_dict["loss"]  # ç”¨lossè¿˜æ˜¯original_loss??
                
                # åå‘ä¼ æ’­
                opt.zero_grad()
                loss.backward()
                opt.step()
                
                total_loss += loss.item()
                batch_count += 1
                
                # if batch_count % 1 == 0:
                #     print(f"   Batch {batch_count}, Loss: {loss.item():.6f}") # print loss in every 1 batch
                
            except Exception as e:
                print(f"   âŒ error in training step: {e}")
                print(f"   batch_data shapes:")
                print(f"     images: {batch_data[0].shape}")
                print(f"     actions: {batch_data[1].shape}")
                print(f"     nonterminals: {batch_data[2].shape}")
                raise e        
        
        # è®¡ç®—æ¯ä¸ªepochçš„å¹³å‡æŸå¤±å¹¶è®°å½•
        if batch_count > 0:
            avg_loss = total_loss / batch_count
            # scheduler.step(avg_loss)
            final_avg_loss = avg_loss  # æ›´æ–°æœ€ç»ˆçš„avg_loss
            
            # æ¯ cfg.data_save_epoch ä¸ªepochæ‰“å°ä¸€æ¬¡æŸå¤±å¹¶è®°å½•åˆ°å†å²
            if (epoch+1) % data_save_epoch == 0:
                loss_history.append(avg_loss)  # åªè®°å½•æ‰“å°çš„æŸå¤±å€¼
                loss_message = f"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.6f}"
                
                logger.info(loss_message)

            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹ï¼Œå¦‚æœæ˜¯ï¼Œä¸”epoch> best_save_intervalï¼Œåˆ™ä¿å­˜æœ€ä½³æ¨¡å‹
                is_best = avg_loss < best_loss
                
                if is_best:
                    # ç«‹å³æ›´æ–°æœ€ä½³æŸå¤±
                    improvement = (best_loss - avg_loss) / best_loss if best_loss != float('inf') else 1.0
                    best_loss = avg_loss
                    best_message = f"This is the new best loss(improvement: {improvement:.2%})"
                    
                    logger.info(best_message)
                    
                    # æ£€æŸ¥æ˜¯å¦åœ¨ä¿å­˜é—´éš”å†…ä¸”æœ‰æ˜¾è‘—æ”¹å–„
                    if (epoch + 1) >= best_save_interval and improvement >= min_improvement:
                        save_best_checkpoint(model, epoch + 1, best_loss, is_best=True, path=cfg.ckpt_path)
                        save_best_message = f"save best model in {cfg.ckpt_path}(improvement: {improvement:.2%})"
                        
                        logger.info(save_best_message)
        
        # æ¯gif_save_epochä¸ªepoch runä¸€æ¬¡test,ä¿å­˜ gif
        if (epoch+1) % gif_save_epoch == 0:
            model_test(cfg.test_img_path, cfg.actions, model, device_obj, cfg.sample_step,epoch+1)

    
    completion_message = "Training completed!"
    print(completion_message)
    logger.info(completion_message)
    

    # è®­ç»ƒå®Œæˆåä¿å­˜æœ€ç»ˆæ¨¡å‹
    if epochs >= 200 and final_avg_loss > 0:
        save_message = "ğŸ’¾ save final training model..."
        print(save_message)
        logger.info(save_message)
        
        save_model(model, epochs, final_avg_loss, path=cfg.ckpt_path)
        
        # è®°å½•è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        stats_message = f"ğŸ“Š training statistics: total epochs: {epochs}, best loss: {best_loss:.6f}, final loss: {final_avg_loss:.6f}, total batches: {batch_count * epochs}"
        print(f"ğŸ“Š training statistics:")
        print(f"    total epochs: {epochs}")
        print(f"    best loss: {best_loss:.6f}")
        print(f"    final loss: {final_avg_loss:.6f}")
        print(f"    total batches: {batch_count * epochs}")
        logger.info(stats_message)
        
        # è®­ç»ƒå®Œæˆåè¿›è¡Œæµ‹è¯•
        model_test(cfg.test_img_path, cfg.actions, model, device_obj, cfg.sample_step,epochs)
    
    # ä¿å­˜æœ€ç»ˆæŸå¤±æ›²çº¿åˆ°outputç›®å½•
    if len(loss_history) > 0:
        final_loss_curve_path = save_loss_curve(loss_history, data_save_epoch, save_path="output")
        logger.info(f"Final loss curve saved to: {final_loss_curve_path}")
    
    # è®°å½•æ—¥å¿—æ–‡ä»¶è·¯å¾„
    final_log_message = f"log path: {log_path}"
    print(final_log_message)
    logger.info(final_log_message)


    


if __name__ == "__main__":
    train()