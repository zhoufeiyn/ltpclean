# 0920 update: try to overfit level1-1 in one directory
# æ›´æ–°: æ·»åŠ äº†æƒé‡ä¿å­˜å’Œç»§ç»­è®­ç»ƒåŠŸèƒ½

"""
ä½¿ç”¨è¯´æ˜:
1. æ­£å¸¸è®­ç»ƒ: åœ¨ config/configTrain.py ä¸­è®¾ç½® resume_training = False
   - ä¼šåŠ è½½é¢„è®­ç»ƒæ¨¡å‹ (cfg.model_path)

2. ç»§ç»­è®­ç»ƒ: åœ¨ config/configTrain.py ä¸­è®¾ç½®:
   - resume_training = True
   - resume_checkpoint_path = "ckpt/model_epoch100_20251018_19.pth"  # æŒ‡å®šè¦åŠ è½½çš„checkpointè·¯å¾„
   - ä¼šä¼˜å…ˆåŠ è½½ç»§ç»­è®­ç»ƒçš„checkpointï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ°é¢„è®­ç»ƒæ¨¡å‹

æƒé‡åŠ è½½ä¼˜å…ˆçº§:
1. ç»§ç»­è®­ç»ƒcheckpoint (åŒ…å«æ¨¡å‹æƒé‡+ä¼˜åŒ–å™¨çŠ¶æ€+è®­ç»ƒä¿¡æ¯)
2. é¢„è®­ç»ƒæ¨¡å‹ (ä»…åŒ…å«æ¨¡å‹æƒé‡)
3. éšæœºåˆå§‹åŒ–æ¨¡å‹

ä¿å­˜çš„æ¨¡å‹åŒ…å«:
- æ¨¡å‹æƒé‡ (network_state_dict)
- ä¼˜åŒ–å™¨çŠ¶æ€ (optimizer_state_dict)
- è®­ç»ƒä¿¡æ¯ (epochs, loss, model_nameç­‰)

è‡ªåŠ¨ä¿å­˜:
- å®šæœŸcheckpoint: model_epoch{epoch}_{timestamp}.pth
"""

from models.vae.sdvae import SDVAE
from algorithm import Algorithm
import torch
import config.configTrain as cfg
import matplotlib.pyplot as plt
import os
from datetime import datetime
from infer_test import model_test
import logging
import random
# å¯¼å…¥æ•°æ®åŠ è½½æ¨¡å—
from dataloader.dataLoad import MarioDataset, build_video_sequence_batch

device: str = "cuda" if torch.cuda.is_available() else "cpu"


# -----------------------------
# Logging Setup
# -----------------------------
def setup_logging():
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    # åˆ›å»ºlogsç›®å½•
    log_dir = "logs"
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)

    # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H")
    log_filename = f"training_log_{timestamp}.log"
    log_path = os.path.join(log_dir, log_filename)

    # é…ç½®æ—¥å¿—æ ¼å¼
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_path, encoding='utf-8'),
            logging.StreamHandler()  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
        ]
    )

    logger = logging.getLogger(__name__)
    logger.info(f"init log: {log_path}")
    return logger, log_path


def save_loss_curve(loss_history, data_save_epoch, save_path="output"):
    """ä¿å­˜æŸå¤±æ›²çº¿å›¾"""
    if not os.path.exists(save_path):
        os.makedirs(save_path)

    # åˆ›å»ºæŸå¤±æ›²çº¿å›¾
    plt.figure(figsize=(10, 6))
    x_epochs = [(i + 1) * data_save_epoch for i in range(len(loss_history))]
    plt.plot(x_epochs, loss_history, 'b-', linewidth=2, label='Training Loss')
    plt.title('Training Loss Curve', fontsize=16)
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss', fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.legend()

    # ä¿å­˜å›¾ç‰‡
    timestamp = datetime.now().strftime("%Y%m%d_%H")
    loss_curve_path = os.path.join(save_path, f"loss_curve_{timestamp}.png")
    plt.savefig(loss_curve_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"ğŸ“ˆ Loss curve saved to: {loss_curve_path}")
    return loss_curve_path


# -----------------------------
# Model Saving Function
# -----------------------------
def save_model(model, optimizer, epochs, final_loss, path=cfg.ckpt_path):
    """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹åˆ°ckptç›®å½•"""

    if not os.path.exists(path):
        os.makedirs(path)

    # ç”Ÿæˆæ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³å’Œepochä¿¡æ¯ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H")
    model_filename = f"model_epoch{epochs}_{timestamp}.pth"
    model_path = os.path.join(path, model_filename)

    # å‡†å¤‡ä¿å­˜çš„æ•°æ®
    save_data = {
        'network_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'epochs': epochs,
        'loss': final_loss,
        'model_name': cfg.model_name,
        'batch_size': cfg.batch_size,
        'num_frames': cfg.num_frames,
        'timestamp': timestamp,
    }

    # ä¿å­˜æ¨¡å‹
    try:
        torch.save(save_data, model_path)
        print(f"âœ… save model to {model_path}")

    except Exception as e:
        print(f"âŒ save model failed: {e}")


def load_model(model, optimizer, checkpoint_path, device_obj):
    """åŠ è½½æ¨¡å‹æƒé‡å’Œä¼˜åŒ–å™¨çŠ¶æ€"""
    if not os.path.exists(checkpoint_path):
        print(f"âš ï¸ Checkpoint not found: {checkpoint_path}")
        return 0, float('inf')

    try:
        print(f"ğŸ“¥ Loading checkpoint: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=device_obj, weights_only=False)

        # åŠ è½½æ¨¡å‹æƒé‡
        model.load_state_dict(checkpoint['network_state_dict'], strict=False)
        print("âœ… Model weights loaded successfully!")

        # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
        if 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            print("âœ… Optimizer state loaded successfully!")
        else:
            print("âš ï¸ No optimizer state found in checkpoint")

        # è·å–è®­ç»ƒä¿¡æ¯
        start_epoch = checkpoint.get('epochs', 0)
        best_loss = checkpoint.get('loss', float('inf'))

        print(f"ğŸ“Š Loaded checkpoint info:")
        print(f"   - Epoch: {start_epoch}")
        print(f"   - Loss: {best_loss:.6f}")
        print(f"   - Model: {checkpoint.get('model_name', 'Unknown')}")

        return start_epoch, best_loss

    except Exception as e:
        print(f"âŒ Failed to load checkpoint: {e}")
        return 0, float('inf')


def vae_encode(batch_data_images, vae_model, device, scale_factor=0.18215):
    """vae encode the images"""
    # å°†å›¾åƒç¼–ç åˆ°æ½œåœ¨ç©ºé—´: [batch_size, num_frames, 3, 128, 128] -> [batch_size, num_frames, 4, 32, 32]
    with torch.no_grad():
        batch_size_videos, num_frames, channels, h, w = batch_data_images.shape
        # é‡å¡‘ä¸º [batch_size * num_frames, 3, 128, 128] è¿›è¡Œæ‰¹é‡ç¼–ç 
        images_flat = batch_data_images.reshape(-1, channels, h, w).to(device)

        # VAEç¼–ç 
        if vae_model is not None:
            latent_dist = vae_model.encode(images_flat)  # [batch_size * num_frames, 3, 128, 128]
            latent_images = latent_dist.sample()  # é‡‡æ ·æ½œåœ¨è¡¨ç¤º [batch_size * num_frames, 4, 32, 32]

            latent_images = latent_images * scale_factor
            # print(f"   Using scale factor: {Config.scale_factor}")

            # é‡å¡‘å› [batch_size, num_frames, 4, 32, 32]
            latent_images = latent_images.reshape(batch_size_videos, num_frames, 4, 32,
                                                  32)  # [batch_size, num_frames, 4, 32, 32]
        else:
            print("âš ï¸ Cannot find VAE model, use original image")
            # å¦‚æœæ²¡æœ‰VAEï¼Œç›´æ¥ä½¿ç”¨åŸå§‹å›¾åƒï¼Œä½†éœ€è¦è°ƒæ•´å½¢çŠ¶
            latent_images = images_flat.reshape(batch_size_videos, num_frames, channels, h, w)
            print(f"   Using original image shape: {latent_images.shape}")

        # æ›´æ–°batch_data[0]ä¸ºç¼–ç åçš„æ½œåœ¨è¡¨ç¤ºï¼Œä¿æŒåœ¨GPUä¸Š
        return latent_images


def train():
    # åˆå§‹åŒ–æ—¥å¿—è®°å½•
    logger, log_path = setup_logging()

    device_obj = torch.device(device)
    # ä½¿ç”¨å¤šè¿›ç¨‹æ•°æ®åŠ è½½ä¼˜åŒ–
    dataset = MarioDataset(cfg.data_path, cfg.img_size, num_workers=8)

    # video sequence parameters
    num_frames = cfg.num_frames
    frame_interval = cfg.frame_interval
    model_name = cfg.model_name

    loss_log_iter = cfg.loss_log_iter
    # gif_save_iter = cfg.gif_save_iter
    gif_save_epoch = cfg.gif_save_epoch
    checkpoint_save_epoch = cfg.checkpoint_save_epoch

    # ä½¿ç”¨Algorithmç±»åŠ è½½å®Œæ•´çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆåŒ…å«VAEå’ŒDiffusionï¼‰
    model = Algorithm(model_name, device_obj)
    model = model.to(device_obj)


    # åˆå§‹åŒ–è®­ç»ƒçŠ¶æ€
    start_epoch = 0
    best_loss = float('inf')

    # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­è®­ç»ƒ - ä¼˜å…ˆåŠ è½½ç»§ç»­è®­ç»ƒçš„checkpoint
    if cfg.resume_training and cfg.resume_checkpoint_path:
        print(f"ğŸ”„ Resuming training from checkpoint: {cfg.resume_checkpoint_path}")
        start_epoch, best_loss = load_model(model, opt, cfg.resume_checkpoint_path, device_obj)
        if start_epoch > 0:
            print(f"âœ… Resuming training from epoch {start_epoch}")
        else:
            print("âš ï¸ Failed to load resume checkpoint, falling back to pretrained model")
            # å¦‚æœç»§ç»­è®­ç»ƒåŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°é¢„è®­ç»ƒæ¨¡å‹
            checkpoint_path = os.path.join(cfg.ckpt_path, cfg.model_path)
            if os.path.exists(checkpoint_path):
                print(f"ğŸ“¥ Loading diffusion forcing pretrained checkpoint: {checkpoint_path}")
                state_dict = torch.load(checkpoint_path, map_location=device_obj, weights_only=False)
                model.load_state_dict(state_dict['network_state_dict'], strict=False)
                print("âœ… diffusion forcing Pretrained checkpoint loaded successfully!")
            else:
                print(
                    f"âš ï¸ diffusion forcing pretrained checkpoint not found: {checkpoint_path}, using random initialized model")
    else:
        # æ²¡æœ‰è®¾ç½®ç»§ç»­è®­ç»ƒï¼ŒåŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        checkpoint_path = os.path.join(cfg.ckpt_path, cfg.model_path)
        if os.path.exists(checkpoint_path):
            print(f"ğŸ“¥ Loading diffusion forcing pretrained checkpoint: {checkpoint_path}")
            state_dict = torch.load(checkpoint_path, map_location=device_obj, weights_only=False)
            model.load_state_dict(state_dict['network_state_dict'], strict=False)
            print("âœ… diffusion forcing pretrained checkpoint loaded successfully!")
        else:
            print(
                f"âš ï¸ diffusion forcing pretrained checkpoint not found: {checkpoint_path}, using random initialized model")
        print("ğŸ†• Starting fresh training")

    # è·å–VAEå’ŒDiffusionæ¨¡å‹
    vae = SDVAE().to(device_obj)
    diffusion_model = model.df_model

    opt = diffusion_model.configure_optimizers_gpt()
    # åˆå§‹åŒ–ä¼˜åŒ–å™¨ - ä½¿ç”¨ç®€å•çš„AdamW
    # opt = torch.optim.AdamW(diffusion_model.parameters(), lr=1e-4, weight_decay=1e-5)
    # åŠ è½½æ‚¨è‡ªå·±è®­ç»ƒçš„VAEæƒé‡
    custom_vae_path = cfg.vae_model
    if custom_vae_path and os.path.exists(custom_vae_path):
        print(f"ğŸ“¥ load your own vae ckpt: {custom_vae_path}")
        custom_state_dict = torch.load(custom_vae_path, map_location=device_obj)
        vae.load_state_dict(custom_state_dict['network_state_dict'], strict=False)
        print("âœ… your vae ckpt loaded successfullyï¼")
    else:
        print("â„¹ï¸ use default pre-trained vae ckpt")

    if vae is not None:
        vae.eval()
        for param in vae.parameters():  # freeze VAE parameters
            param.requires_grad = False
        print("âœ… VAE already loadedï¼ŒVAE parameters has been frozen")
    else:
        print("âš ï¸ Cannot find VAE model")
    epochs, batch_size = cfg.epochs, cfg.batch_size

    print("---1. start training----")
    print("---2. load dataset---")
    total_samples = len(dataset)
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®
    if total_samples < num_frames:
        print(f"âŒ dataset not enough: need at least {num_frames} samples, but only {total_samples} samples")
        return
    # è®¡ç®—å¯ä»¥åˆ›å»ºå¤šå°‘ä¸ªå®Œæ•´çš„è§†é¢‘åºåˆ—
    num_videos = (total_samples - num_frames) // frame_interval + 1
    print(f"dataset loaded: {total_samples} samples, construct {num_videos} complete video sequences, "
          f"each video has {num_frames} frames, construct {(num_videos + batch_size - 1) // batch_size} batches, the batch size is {batch_size}")

    # åˆå§‹åŒ–æŸå¤±å†å²è®°å½•
    loss_history = []
    final_avg_loss = 0  # ç”¨äºä¿å­˜æœ€ç»ˆçš„avg_loss

    # é¢„è®¡ç®—æ‰€æœ‰æœ‰æ•ˆçš„è§†é¢‘åºåˆ—èµ·å§‹ä½ç½®,é—´éš”ä¸€ä¸ªframe_intervalå–ä¸€ä¸ªvideo sequence, æœ€ç»ˆå‰©ä¸‹ä¸è¶³ä¸€ä¸ªvideoçš„æ‰”æ‰
    valid_starts = []
    for start in range(0, total_samples - num_frames + 1, frame_interval):
        valid_starts.append(start)

    # æŒ‰batch_sizeåˆ†ç»„å¤„ç†
    num_valid_videos = len(valid_starts)

    for epoch in range(start_epoch, epochs):
        total_loss = 0
        batch_count = 0

        # ğŸ”¥ æ¯ä¸ªepochå¼€å§‹æ—¶shuffleè§†é¢‘åºåˆ—é¡ºåº
        shuffled_valid_starts = valid_starts.copy()
        random.shuffle(shuffled_valid_starts)

        # æŒ‰batchå¤„ç† - ä¼˜åŒ–ç‰ˆæœ¬
        for batch_start in range(0, num_valid_videos, batch_size):
            batch_end = min(batch_start + batch_size, num_valid_videos)
            current_batch_size = batch_end - batch_start

            # è·å–å½“å‰batchçš„èµ·å§‹ç´¢å¼•ï¼ˆç°åœ¨æ˜¯shuffledçš„ï¼‰
            current_start_indices = shuffled_valid_starts[batch_start:batch_end]

            # æ‰¹é‡æ„å»ºè§†é¢‘åºåˆ—
            batch_images, batch_actions, batch_nonterminals = build_video_sequence_batch(dataset, current_start_indices, num_frames)

            # å¦‚æœbatchä¸æ»¡ï¼Œç”¨æœ€åä¸€ä¸ªè§†é¢‘å¤åˆ¶è¡¥é½
            if current_batch_size < batch_size:
                last_video_images = batch_images[-1]
                last_video_actions = batch_actions[-1]
                last_video_nonterminals = batch_nonterminals[-1]

                for _ in range(batch_size - current_batch_size):
                    batch_images.append(last_video_images)
                    batch_actions.append(last_video_actions)
                    batch_nonterminals.append(last_video_nonterminals)

            # æ‹¼æ¥æˆbatch_tensor
            batch_data = [
                torch.cat(batch_images, dim=0).to(device_obj),
                torch.cat(batch_actions, dim=0).to(device_obj),
                torch.cat(batch_nonterminals, dim=0).to(device_obj)
            ]

            batch_data[0] = vae_encode(batch_data[0], vae, device_obj)

            # # for small dataset æ‰©å±•batch_size: [b, num_frames, channels, h, w] -> [b*16, num_frames, channels, h, w]
            batch_data[0] = batch_data[0].repeat(156, 1, 1, 1, 1)

            # åŒæ­¥æ‰©å±•actionså’Œnonterminals
            batch_data[1] = batch_data[1].repeat(156, 1, 1)  # actions: [1, num_frames, 1] -> [16, num_frames, 1]
            batch_data[2] = batch_data[2].repeat(156, 1)  # nonterminals: [1, num_frames] -> [16, num_frames]

            # è®­ç»ƒæ­¥éª¤
            try:
                out_dict = diffusion_model.training_step(batch_data)
                loss = out_dict["loss"]  # ç”¨lossè¿˜æ˜¯original_loss??

                # åå‘ä¼ æ’­
                opt.zero_grad()
                loss.backward()
                opt.step()

                total_loss += loss.item()
                batch_count += 1

                # if batch_count % 1 == 0:
                #     print(f"   Batch {batch_count}, Loss: {loss.item():.6f}") # print loss in every 1 batch

            except Exception as e:
                print(f"   âŒ error in training step: {e}")
                print(f"   batch_data shapes:")
                print(f"     images: {batch_data[0].shape}")
                print(f"     actions: {batch_data[1].shape}")
                print(f"     nonterminals: {batch_data[2].shape}")
                raise e

            # æŸ¥çœ‹batché‡Œçš„losså’Œ gif
            if batch_count % loss_log_iter == 0:
                batch_loss = loss.item()
                loss_message = f"Epoch {epoch + 1}/{epochs}, in batch: {batch_count},  Loss: {batch_loss:.6f}"
                logger.info(loss_message)

        # ä¸€ä¸ªepoch
        if batch_count > 0:
            avg_loss = total_loss / batch_count
            # scheduler.step(avg_loss)
            final_avg_loss = avg_loss  # æ›´æ–°æœ€ç»ˆçš„avg_loss
            # æ¯ 1 ä¸ªepochæ‰“å°ä¸€æ¬¡æŸå¤±å¹¶è®°å½•åˆ°å†å²
            loss_history.append(avg_loss)  # åªè®°å½•æ‰“å°çš„æŸå¤±å€¼
            loss_message = f"Epoch {epoch + 1}/{epochs}, Average Loss: {avg_loss:.6f}"
            logger.info(loss_message)
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™ä¿å­˜æœ€ä½³æ¨¡å‹
            is_best = avg_loss < best_loss
            if is_best:
                # ç«‹å³æ›´æ–°æœ€ä½³æŸå¤±
                improvement = (best_loss - avg_loss) / best_loss if best_loss != float('inf') else 1.0
                best_loss = avg_loss
                best_message = f"This is the new best loss(improvement: {improvement:.2%})"
                logger.info(best_message)

        # æ¯gif_save_epochä¸ªepoch runä¸€æ¬¡test,ä¿å­˜ gif
        if (epoch + 1) % gif_save_epoch == 0:
            # ç¡®ä¿outputç›®å½•å­˜åœ¨

            model_test(cfg.test_img_path1, cfg.actions1, model, vae, device_obj, cfg.sample_step,
                       f'{cfg.test_img_path1[-9:-4]}_epoch{epoch + 1}_r', epoch=epoch + 1, output_dir='output')
            # model_test(cfg.test_img_path1, cfg.actions2, model, vae, device_obj, cfg.sample_step,
            #            f'{cfg.test_img_path1[-9:-4]}_epoch{epoch + 1}_rj', epoch=epoch + 1, output_dir='output')

        # æ¯checkpoint_save_epochä¸ªepochä¿å­˜ä¸€æ¬¡checkpoint
        if (epoch + 1) % checkpoint_save_epoch == 0:
            current_loss = avg_loss if batch_count > 0 else 0
            save_model(model, opt, epoch + 1, current_loss, path=cfg.ckpt_path)
            checkpoint_message = f"ğŸ’¾ Checkpoint saved at epoch {epoch + 1}"
            logger.info(checkpoint_message)

    completion_message = "Training completed!"
    print(completion_message)
    logger.info(completion_message)

    # è®­ç»ƒå®Œæˆåä¿å­˜æœ€ç»ˆæ¨¡å‹
    if epochs >= 1 and final_avg_loss > 0:
        save_message = "ğŸ’¾ save final training model..."
        print(save_message)
        logger.info(save_message)

        save_model(model, opt, epochs, final_avg_loss, path=cfg.ckpt_path)

        # è®°å½•è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        stats_message = f"ğŸ“Š training statistics: total epochs: {epochs}, best loss: {best_loss:.6f}, final loss: {final_avg_loss:.6f}, total batches: {batch_count * epochs}"
        print(f"ğŸ“Š training statistics:")
        print(f"    total epochs: {epochs}")
        print(f"    best loss: {best_loss:.6f}")
        print(f"    final loss: {final_avg_loss:.6f}")
        print(f"    total batches: {batch_count * epochs}")
        logger.info(stats_message)

        # è®­ç»ƒå®Œæˆåè¿›è¡Œæµ‹è¯•
        model_test(cfg.test_img_path1, cfg.actions1, model, vae, device_obj, cfg.sample_step,
                   f'{cfg.test_img_path1[-9:-4]}_result_{epochs}_r', epoch='result', output_dir='output')
        # model_test(cfg.test_img_path1, cfg.actions2, model, vae, device_obj, cfg.sample_step,
        #            f'{cfg.test_img_path1[-9:-4]}_result_{epochs}_rj', epoch='result', output_dir='output')

    # ä¿å­˜æœ€ç»ˆæŸå¤±æ›²çº¿åˆ°outputç›®å½•
    if len(loss_history) > 0:
        final_loss_curve_path = save_loss_curve(loss_history, 1, save_path="output")
        logger.info(f"Final loss curve saved to: {final_loss_curve_path}")

    # è®°å½•æ—¥å¿—æ–‡ä»¶è·¯å¾„
    final_log_message = f"log path: {log_path}"
    print(final_log_message)
    logger.info(final_log_message)


if __name__ == "__main__":
    train()