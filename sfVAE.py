from torchvision.transforms import InterpolationMode

from models.vae.sdvae import SDVAE
import torch
import config.configVAE as cfg
import torch.nn.functional as F
import matplotlib.pyplot as plt
import os
from datetime import datetime

from dataloader.dataLoad import MarioDataset
from train import setup_logging, save_loss_curve

def build_img_batch_from_indices(dataset, indices):
    """æ ¹æ®ç´¢å¼•åˆ—è¡¨æ„å»ºå›¾ç‰‡æ‰¹æ¬¡"""
    batch_images = []
    for idx in indices:
        image, _, _ = dataset[idx]
        batch_images.append(image)
    return torch.stack(batch_images, dim=0)

device: str = "cuda" if torch.cuda.is_available() else "cpu"

def save_model_with_optimizer(model, optimizer, epochs, final_loss, best_loss, loss_history, path=cfg.ckpt_path):
    """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹åˆ°ckptç›®å½•ï¼ŒåŒ…å«ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨çŠ¶æ€"""
    if not os.path.exists(path):
        os.makedirs(path)

    # ç”Ÿæˆæ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³å’Œepochä¿¡æ¯ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H")
    model_filename = f"vae_epoch{epochs}_{timestamp}.pth"
    model_path = os.path.join(path, model_filename)

    # å‡†å¤‡ä¿å­˜çš„æ•°æ®
    save_data = {
        'network_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'epoch': epochs,
        'loss': final_loss,
        'best_loss': best_loss,
        'loss_history': loss_history,
        'model_name': 'SDVAE',
        'batch_size': cfg.batch_size,
    }

    # ä¿å­˜æ¨¡å‹
    try:
        torch.save(save_data, model_path)
        print(f"âœ… VAE model saved to {model_path}")

    except Exception as e:
        print(f"âŒ Save VAE model failed: {e}")

def infer_test(img):
    device_obj = torch.device(device)
    model = SDVAE()
    
    # åªæœ‰å½“ model_path ä¸ä¸ºç©ºæ—¶æ‰æ‹¼æ¥è·¯å¾„
    if cfg.model_path:
        ckpt_path = os.path.join(cfg.ckpt_path, cfg.model_path)
    else:
        ckpt_path = cfg.ckpt_path
    
    if os.path.exists(ckpt_path):
        print(f"ğŸ“¥ load pretrained checkpoint: {ckpt_path}")
        state_dict = torch.load(ckpt_path, map_location=device_obj, weights_only=False)
        model.load_state_dict(state_dict['network_state_dict'], strict=False)
        print("ckpt loaded successfully")
    else:
        print(f"âš ï¸ Checkpoint not found: {ckpt_path}, use initialized model")
    
    model = model.to(device_obj)
    vae_test(img, model, device_obj, 'infer')





def vae_test(img_path, model, device_obj, e=None, out_dir='output/VAE' ):
    """æµ‹è¯•VAEæ¨¡å‹çš„ç¼–ç è§£ç æ•ˆæœ"""
    import os
    import glob
    from PIL import Image
    import torchvision.transforms as transforms
    import torch.nn.functional as F

    if not os.path.exists(out_dir):
        os.makedirs(out_dir)
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = out_dir+f"/epoch{e+1}"
    os.makedirs(output_dir, exist_ok=True)
    
    # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
    if os.path.isfile(img_path):
        img_files = [img_path]
    else:
        img_files = glob.glob(os.path.join(img_path, "*.png")) + glob.glob(os.path.join(img_path, "*.jpg"))
    
    if not img_files:
        print(f"âŒ No images found in {img_path}")
        return
    
    model.eval()
    total_loss = 0
    num_images = 0
    
    # å®šä¹‰å›¾åƒå˜æ¢
    transform = transforms.Compose([
        transforms.Resize((256, 256),interpolation=InterpolationMode.NEAREST),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])
    
    with torch.no_grad():
        for img_file in img_files[:10]:  # é™åˆ¶æµ‹è¯•å›¾ç‰‡æ•°é‡
            try:
                # åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
                img = Image.open(img_file).convert('RGB')
                img_tensor = transform(img).unsqueeze(0).to(device_obj)
                
                # VAEç¼–ç è§£ç 
                encoded = model.encode(img_tensor)
                latent = encoded.sample()
                decoded = model.decode(latent)
                
                # è®¡ç®—é‡å»ºæŸå¤±
                loss = F.l1_loss(decoded, img_tensor)
                total_loss += loss.item()
                num_images += 1
                
                # ä¿å­˜åŸå§‹å›¾åƒå’Œé‡å»ºå›¾åƒ
                img_name = os.path.splitext(os.path.basename(img_file))[0]
                
                # è½¬æ¢ä¸ºå¯ä¿å­˜çš„æ ¼å¼
                original_img = (img_tensor[0].cpu() * 0.5 + 0.5).clamp(0, 1)
                reconstructed_img = (decoded[0].cpu() * 0.5 + 0.5).clamp(0, 1)
                
                # ä¿å­˜å›¾åƒ
                transforms.ToPILImage()(original_img).save(os.path.join(output_dir, f"{img_name}_original.png"))
                transforms.ToPILImage()(reconstructed_img).save(os.path.join(output_dir, f"{img_name}_reconstructed.png"))
                
            except Exception as ex:
                print(f"âŒ Error processing {img_file}: {ex}")
                continue
    
    if num_images > 0:
        avg_loss = total_loss / num_images
        print(f"âœ… VAE test completed. Average reconstruction loss: {avg_loss:.6f}")
        print(f"ğŸ“ Test images saved to: {output_dir}")
    else:
        print("âŒ No images were successfully processed")
    
    model.train()  # æ¢å¤è®­ç»ƒæ¨¡å¼


def estimate_scaling_factor():
    target_std = 1.0

    device_obj = torch.device(device)
    # ä½¿ç”¨å¤šè¿›ç¨‹æ•°æ®åŠ è½½ä¼˜åŒ–
    dataset = MarioDataset(cfg.data_path, cfg.img_size, num_workers=8)
    model = SDVAE().to(device_obj)
    
    # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    if cfg.model_path:
        ckpt_path = os.path.join(cfg.ckpt_path, cfg.model_path)
    else:
        ckpt_path = cfg.ckpt_path
    
    if os.path.exists(ckpt_path):
        print(f"ğŸ“¥ Loading pretrained checkpoint: {ckpt_path}")
        state_dict = torch.load(ckpt_path, map_location=device_obj, weights_only=False)
        model.load_state_dict(state_dict['network_state_dict'], strict=False)
        print("âœ… Checkpoint loaded successfully")
    else:
        print(f"âš ï¸ Checkpoint not found: {ckpt_path}, using initialized model")
    
    model.eval()
    std_list = []
    
    # ä½¿ç”¨å…¨éƒ¨æ•°æ®è¿›è¡Œç»Ÿè®¡
    total_samples = len(dataset)
    batch_size = cfg.batch_size
    
    print(f"ğŸ“Š Computing scaling factor using {total_samples} samples")
    print(f"ğŸ“Š Batch size: {batch_size}")

    with torch.no_grad():
        for batch_idx in range(0, total_samples, batch_size):
            # æ„å»ºæ‰¹æ¬¡ç´¢å¼•
            end_idx = min(batch_idx + batch_size, total_samples)
            batch_indices = list(range(batch_idx, end_idx))
            
            # æ„å»ºæ‰¹æ¬¡æ•°æ®
            batch_img = build_img_batch_from_indices(dataset, batch_indices).to(device_obj)
            
            # VAEå‰å‘ä¼ æ’­
            encoded = model.encode(batch_img).sample()
            std_list.append(encoded.std().item())
            
            if batch_idx % (batch_size * 10) == 0:
                print(f"Processed {batch_idx}/{total_samples} samples")
    
    avg_std = sum(std_list) / len(std_list)
    scaling = target_std / avg_std
    
    print(f"ğŸ“Š Computed scaling factor:")
    print(f"   Average latent std: {avg_std:.6f}")
    print(f"   Target std: {target_std:.6f}")
    print(f"   Recommended scaling factor: {scaling:.6f}")
    
    return scaling




if __name__ == "__main__":
    estimate_scaling_factor()